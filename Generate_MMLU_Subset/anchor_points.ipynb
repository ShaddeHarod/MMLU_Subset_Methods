{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Finding and using anchor points\n",
    "\n",
    "In this notebook, we show how to find anchor points based on your training set and how to use them to estimate the performance of new models in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d9b93-5059-416c-8a53-e3b4cc24a904",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from irt import *\n",
    "from utils import *\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8ce2-1851-4131-8d35-36214be71085",
   "metadata": {},
   "source": [
    "The leaderboard dataset we will use is composed by six scenarios (sub-datasets):\n",
    "1. TruthfulQA\n",
    "1. GSM8K\n",
    "1. Winogrande\n",
    "1. ARC\n",
    "1. HellaSwag\n",
    "1. MMLU\n",
    "\n",
    "MMLU is further divided into sub-scenarios (e.g., abstract algebra, anatomy, etc). Let's check scenarios and sub-scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26499fc1-2bda-44b2-9131-e78d16f7f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness_truthfulqa_mc_0': ['harness_truthfulqa_mc_0'],\n",
       " 'gsm8k': ['harness_gsm8k_5'],\n",
       " 'winogrande': ['harness_winogrande_5'],\n",
       " 'arc': ['harness_arc_challenge_25'],\n",
       " 'hellaswag': ['harness_hellaswag_10'],\n",
       " 'mmlu': ['harness_hendrycksTest_abstract_algebra_5',\n",
       "  'harness_hendrycksTest_anatomy_5',\n",
       "  'harness_hendrycksTest_astronomy_5',\n",
       "  'harness_hendrycksTest_business_ethics_5',\n",
       "  'harness_hendrycksTest_clinical_knowledge_5',\n",
       "  'harness_hendrycksTest_college_biology_5',\n",
       "  'harness_hendrycksTest_college_chemistry_5',\n",
       "  'harness_hendrycksTest_college_computer_science_5',\n",
       "  'harness_hendrycksTest_college_mathematics_5',\n",
       "  'harness_hendrycksTest_college_medicine_5',\n",
       "  'harness_hendrycksTest_college_physics_5',\n",
       "  'harness_hendrycksTest_computer_security_5',\n",
       "  'harness_hendrycksTest_conceptual_physics_5',\n",
       "  'harness_hendrycksTest_econometrics_5',\n",
       "  'harness_hendrycksTest_electrical_engineering_5',\n",
       "  'harness_hendrycksTest_elementary_mathematics_5',\n",
       "  'harness_hendrycksTest_formal_logic_5',\n",
       "  'harness_hendrycksTest_global_facts_5',\n",
       "  'harness_hendrycksTest_high_school_biology_5',\n",
       "  'harness_hendrycksTest_high_school_chemistry_5',\n",
       "  'harness_hendrycksTest_high_school_computer_science_5',\n",
       "  'harness_hendrycksTest_high_school_european_history_5',\n",
       "  'harness_hendrycksTest_high_school_geography_5',\n",
       "  'harness_hendrycksTest_high_school_government_and_politics_5',\n",
       "  'harness_hendrycksTest_high_school_macroeconomics_5',\n",
       "  'harness_hendrycksTest_high_school_mathematics_5',\n",
       "  'harness_hendrycksTest_high_school_microeconomics_5',\n",
       "  'harness_hendrycksTest_high_school_physics_5',\n",
       "  'harness_hendrycksTest_high_school_psychology_5',\n",
       "  'harness_hendrycksTest_high_school_statistics_5',\n",
       "  'harness_hendrycksTest_high_school_us_history_5',\n",
       "  'harness_hendrycksTest_high_school_world_history_5',\n",
       "  'harness_hendrycksTest_human_aging_5',\n",
       "  'harness_hendrycksTest_human_sexuality_5',\n",
       "  'harness_hendrycksTest_international_law_5',\n",
       "  'harness_hendrycksTest_jurisprudence_5',\n",
       "  'harness_hendrycksTest_logical_fallacies_5',\n",
       "  'harness_hendrycksTest_machine_learning_5',\n",
       "  'harness_hendrycksTest_management_5',\n",
       "  'harness_hendrycksTest_marketing_5',\n",
       "  'harness_hendrycksTest_medical_genetics_5',\n",
       "  'harness_hendrycksTest_miscellaneous_5',\n",
       "  'harness_hendrycksTest_moral_disputes_5',\n",
       "  'harness_hendrycksTest_moral_scenarios_5',\n",
       "  'harness_hendrycksTest_nutrition_5',\n",
       "  'harness_hendrycksTest_philosophy_5',\n",
       "  'harness_hendrycksTest_prehistory_5',\n",
       "  'harness_hendrycksTest_professional_accounting_5',\n",
       "  'harness_hendrycksTest_professional_law_5',\n",
       "  'harness_hendrycksTest_professional_medicine_5',\n",
       "  'harness_hendrycksTest_professional_psychology_5',\n",
       "  'harness_hendrycksTest_public_relations_5',\n",
       "  'harness_hendrycksTest_security_studies_5',\n",
       "  'harness_hendrycksTest_sociology_5',\n",
       "  'harness_hendrycksTest_us_foreign_policy_5',\n",
       "  'harness_hendrycksTest_virology_5',\n",
       "  'harness_hendrycksTest_world_religions_5']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e5620-bd45-4985-b390-a154843b4d6c",
   "metadata": {},
   "source": [
    "Loading leaderboard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca68f5c-49de-4f75-92e5-de639059cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lb.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14f180-d322-4cd5-8d0c-4ae4fef04127",
   "metadata": {},
   "source": [
    "In this dataset, we have data from 395 models. Let's see the names of some of them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6c4201-0675-42e5-8a7a-8cf75592e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395,\n",
       " ['open-llm-leaderboard/details_zhengr__MixTAO-7Bx2-MoE-DPO',\n",
       "  'open-llm-leaderboard/details_alignment-handbook__zephyr-7b-sft-full',\n",
       "  'open-llm-leaderboard/details_rombodawg__Leaderboard-killer-MoE_4x7b',\n",
       "  'open-llm-leaderboard/details_FelixChao__ExtremeDolphin-MoE',\n",
       "  'open-llm-leaderboard/details_LoSboccacc__orthogonal-2x7B-base',\n",
       "  'open-llm-leaderboard/details_moreh__MoMo-70B-lora-1.8.6-DPO',\n",
       "  'open-llm-leaderboard/details_deepseek-ai__deepseek-moe-16b-base',\n",
       "  'open-llm-leaderboard/details_Swisslex__Mixtral-Orca-v0.1',\n",
       "  'open-llm-leaderboard/details_wang7776__Mistral-7B-Instruct-v0.2-sparsity-20',\n",
       "  'open-llm-leaderboard/details_nfaheem__Marcoroni-7b-DPO-Merge'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['models']),data['models'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8135b-e9ec-468a-85a7-3cd3fc3d31fe",
   "metadata": {},
   "source": [
    "Below, we will process the data so all correctness scores (for all scenarios) are stored in $Y$. The dictionaries `scenarios_position` and `subscenarios_position` give the position of scenarios/subscenarios correctness scores in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee09c25b-2dc4-4403-a972-9fb05cfe917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 28659)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002485a-1e82-409b-aaf2-ddb6a82bc315",
   "metadata": {},
   "source": [
    "For example, below you can see the scores for MMLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4dd9649-ba75-49c0-92fe-b00d2afc252e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [1., 0., 1., ..., 1., 1., 0.]]),\n",
       " (395, 14042))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:,scenarios_position['mmlu']], Y[:,scenarios_position['mmlu']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662681a7-10b0-4ddc-a692-52d278499539",
   "metadata": {},
   "source": [
    "For scenarios that have multiple subscenarios, it is usually the case that we want to give equal importance to individual subscenarios when computing the aggregated performance in that scenario. This is equivalent to using a weighted average when computing the aggregated performance. We will create `balance_weights`, a vector of weights to help us compute those weighted averages. These weights will be different than one only for MMLU, which is the only scenario with multiple subscenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40fc53-b11e-41cc-adc2-7abff1a2b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28659 包含包括mmlu所有场景的子集，和其他的数据集的subscenario\n",
    "balance_weights = np.ones(Y.shape[1])\n",
    "# N为MMLU的问题总数\n",
    "N = len(scenarios_position['mmlu'])\n",
    "# n_sub为mmlu科目数量\n",
    "n_sub = len(scenarios['mmlu'])\n",
    "# sub为科目\n",
    "for sub in scenarios['mmlu']:\n",
    "    # n_i为对应subject题目数量\n",
    "    n_i = len(subscenarios_position['mmlu'][sub])\n",
    "    # idx = subscenario_position['mmlu'][sub],是mmlu子集在所有subscenrio中的位置\n",
    "    # n_sub * n_i 为对应subject的问题数量，乘上mmlu科目数量（57）。balance_weights中有大于1有小于1的，大于1说明科目数量小，给予更高的weight\n",
    "    balance_weights[subscenarios_position['mmlu'][sub]] = N/(n_sub*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d42b7-c6ac-4695-a5f0-3087c091d16d",
   "metadata": {},
   "source": [
    "We can see below that first averaging within subscenarios and then computing a simple average is equivalent to using a weighted average from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b51b6f-5ce5-46bf-ba44-836386db05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.322333605307685e-14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accs1 先计算每个模型同一个科目的总准确率，再把每个模型的所有科目的准确率汇总等权，计算每个模型的准确率，形成(395, )的准确率\n",
    "accs1 = np.mean([Y[:,subscenarios_position['mmlu'][sub]].mean(axis=1) for sub in scenarios['mmlu']], axis=0)\n",
    "# balance_weights*Y，每行大模型的0/1正确率都会乘上对应的weight。scenarios_position['mmlu']取14042个问题对应的index。mean后为每个模型的总准确率，形状为（395,）\n",
    "accs2 = (balance_weights*Y)[:,scenarios_position['mmlu']].mean(axis=1)\n",
    "# 两者结果一致\n",
    "np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106b620-7fe0-49bb-a8ac-3a946c15f751",
   "metadata": {},
   "source": [
    "## Getting and using anchor points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c4412-ae69-4184-b106-191a1c151736",
   "metadata": {},
   "source": [
    "Let's split the data in train and test (recent models are placed in the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9874c5-7cb5-425b-8c41-9a87d7615ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y第一个维度为模型\n",
    "Y_test = Y[:100]\n",
    "Y_train = Y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7865657803785115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准确率最高的模型的准确率\n",
    "(balance_weights*Y_train)[:,scenarios_position['mmlu']].mean(axis=1).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680d6e6-1ec2-4a24-a898-f29bd5ec109e",
   "metadata": {},
   "source": [
    "The variable `number_item` gives the number of anchor points we want to find in each scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a80e3b-8e0c-402f-8263-7e178b976bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_item = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f89a-64bb-497e-b993-9181996d75d1",
   "metadata": {},
   "source": [
    "The variable `clustering` specified how the clusting is run. If `clustering=\"correct.\"`, then correctness is used. On the other hand, if `clustering=\"irt\"`, then the IRT embeddings for examples are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5edf1d-21b3-46f9-9034-479ebe89314d",
   "metadata": {},
   "source": [
    "Computing anchor points and their weights for each scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c85b8-838d-416c-ac7d-e40725e08853",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_points = {}\n",
    "anchor_weights = {}\n",
    "\n",
    "for scenario in scenarios.keys():\n",
    "    # X 若为correct cluster，则每行为mmlu题目，每列为模型回答的0/1的答案，列数为模型数\n",
    "    # X 若为irt cluster，则每行为mmlu题目，每列为题目区分度A和难度B，列数为2\n",
    "    if clustering=='correct.':\n",
    "        X = Y_train[:,scenarios_position[scenario]].T\n",
    "    elif clustering=='irt':\n",
    "        #这行代码会从预先训练好的 IRT 模型中加载每个题目的两个核心参数：\n",
    "        # `A` (Discrimination/区分度): 表示一个题目在区分高能力和低能力模型上的效果有多好。A 值越高，区分度越好。\n",
    "        # `B` (Difficulty/难度): 表示一个题目的难度值。B 值越高，题目越难。\n",
    "        A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "        # 首先，它将所有题目的 A 和 B 参数合并成一个矩阵。转置（.T）后，这个矩阵的每一行代表一道题目，而两列分别是这道题的区分度（A）和难度（B）。\n",
    "        X = np.vstack((A.squeeze(), B.squeeze().reshape((1,-1)))).T\n",
    "        # 然后，它从这个总矩阵中筛选出当前 scenario (例如 'mmlu') 所对应的那些题目。\n",
    "        X = X[scenarios_position[scenario]]\n",
    "    else:\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    #Normalizing balance_weights, so their sum is one within each scenario\n",
    "    norm_balance_weights = balance_weights[scenarios_position[scenario]]\n",
    "    norm_balance_weights /= norm_balance_weights.sum()\n",
    "\n",
    "    # Fitting the KMeans model\n",
    "    # * kmeans.labels_:\n",
    "    #   * 内容: 一个一维数组，长度与 X 的行数（即题目数量）相同。数组中的第 i 个值，就是第 i 道题目被分配到的簇的编号（从 0\n",
    "    #     到 99）。\n",
    "    #   * 作用: 这是最直接的聚类结果，告诉我们每道题属于哪个簇。\n",
    "\n",
    "    #* kmeans.cluster_centers_:\n",
    "    #   * 内容: 一个形状为 (100, 特征数量) 的二维数组。每一行代表一个簇的中心点（质心）在特征空间中的坐标。\n",
    "    #   * 作用: 代表了 100 个簇的“平均”特征。后续代码会用它来寻找离每个中心点最近的真实题目，作为“锚点”。\n",
    "\n",
    "    #* kmeans.inertia_:\n",
    "    #   * 内容: 一个浮点数，表示所有样本点到其所属簇中心的距离平方和。\n",
    "    #   * 作用: 它是衡量聚类效果的一个指标，值越小通常表示聚类效果越好（簇内更紧密）。\n",
    "    kmeans = KMeans(n_clusters=number_item, n_init=\"auto\", random_state=random_state)\n",
    "    kmeans.fit(X, sample_weight=norm_balance_weights)\n",
    "\n",
    "    # Calculating anchor points\n",
    "    # 对于 KMeans 算法找到的 100 个簇，分别计算出离每个簇的中心点最近的那道真实题目，并将这 100道真实题目的索引作为该场景下的“锚点”保存下来。\n",
    "    # 这些“锚点”就是对整个题库进行浓缩和降维后得到的、最具代表性的题目样本。\n",
    "    anchor_points[scenario] = pairwise_distances(kmeans.cluster_centers_, X, metric='euclidean').argmin(axis=1)\n",
    "\n",
    "    # Calculating anchor weights\n",
    "    # 对number_item个簇，计算每个簇的权重，权重为该簇内所有题目权重的和\n",
    "    # kmeans.labels_ == c:\n",
    "    #   * 这是一个布尔判断。对于当前的簇编号 c，它会生成一个布尔类型的“掩码” (mask) 数组。\n",
    "    #   * 例如，当 c=5 时，这个掩码数组中，所有属于第 5 簇的题目的位置为 True，其他都为 False。\n",
    "    # norm_balance_weights[...]:\n",
    "    #   * norm_balance_weights 是一个数组，包含了每一道题目的归一化权重。\n",
    "    #   * norm_balance_weights[kmeans.labels_==c] 这个操作利用上面的布尔掩码，从 norm_balance_weights 中只挑选出那些属于簇`c` 的题目的权重。\n",
    "    anchor_weights[scenario] = np.array([np.sum(norm_balance_weights[kmeans.labels_==c]) for c in range(number_item)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c071b44-1410-4cb8-9d20-2f5a3ed9c5c9",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651edaf8-af28-4e6f-92d6-192477c0aa44",
   "metadata": {},
   "source": [
    "Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11008304-b6db-4b55-863d-c9968a0b76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = {'anchor_points':anchor_points,\n",
    "          'anchor_weights':anchor_weights}\n",
    "\n",
    "with open('data/anchor.pickle', 'wb') as handle:\n",
    "    pickle.dump(anchor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a468b8f-950b-41c5-b009-e9b3d913b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3339,   216,   186,  4895,  4826,  8082,  9644, 13748, 11767,\n",
       "        9610,  1215,  2854,   627,  5853, 11538,  5943,  4801, 11465,\n",
       "        1880, 10257,  2354,  4882, 10627,  6597, 12963, 11964, 11970,\n",
       "        7769,  2698,  5738, 13289,  9662, 12060,  8037,  3175,  2051,\n",
       "        8692, 11950,    64,  2662,  9632,  5856,  4725,  5934,   614,\n",
       "        2282,  2762,  4608,  9444,  7792,  9833,  7577,  8167,  8102,\n",
       "        5561,  3233, 11120,  4194,  6624,   739,  5516,  1631, 10081,\n",
       "        3889,   964,  5119,  2028,  8744,  4244,  4022,  9671, 12699,\n",
       "       11920,  3439,  5151,  3237,  9447, 12142,  2268,   142,  1188,\n",
       "        8422, 12505,  6321,  1557,  6551,  6380,  7861, 10460,  7677,\n",
       "        6877,  1384,  5460,   571,  3356,  8524, 11024,  4790,  1059,\n",
       "       11589], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_points['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6955cb1-de2d-4346-a12e-4356e464c2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00719509, 0.02925134, 0.00688233, 0.01077535, 0.01135904,\n",
       "       0.00979226, 0.00669851, 0.01577534, 0.00868074, 0.01017981,\n",
       "       0.00669675, 0.00840938, 0.00837327, 0.01174739, 0.01084033,\n",
       "       0.01212135, 0.008107  , 0.00751334, 0.01068052, 0.01132448,\n",
       "       0.00710094, 0.00855757, 0.00723255, 0.00838955, 0.00879337,\n",
       "       0.00646765, 0.00925892, 0.01495362, 0.01062908, 0.01159708,\n",
       "       0.00860773, 0.01295747, 0.00854535, 0.02359755, 0.00576909,\n",
       "       0.00759589, 0.00455757, 0.01114833, 0.00522446, 0.01094169,\n",
       "       0.01302504, 0.01764216, 0.00971956, 0.00580515, 0.00925852,\n",
       "       0.00768513, 0.00876491, 0.01582696, 0.0079103 , 0.01071352,\n",
       "       0.01975156, 0.00609008, 0.00780274, 0.01213502, 0.00831919,\n",
       "       0.00980854, 0.00854498, 0.00879001, 0.02524748, 0.00838861,\n",
       "       0.01593213, 0.01020836, 0.0090561 , 0.00836418, 0.00673287,\n",
       "       0.00647885, 0.00727505, 0.00704453, 0.00873509, 0.0079446 ,\n",
       "       0.00953117, 0.00639431, 0.00897941, 0.00801886, 0.01038038,\n",
       "       0.00838688, 0.00945537, 0.00906167, 0.00731631, 0.00840227,\n",
       "       0.00845057, 0.01091929, 0.00945898, 0.01808568, 0.00965281,\n",
       "       0.00862978, 0.0096213 , 0.00602213, 0.00734227, 0.02271256,\n",
       "       0.0090485 , 0.00914891, 0.00746462, 0.01083964, 0.01006115,\n",
       "       0.00716084, 0.00657862, 0.00850945, 0.01298723, 0.00804871])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_weights['mmlu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac7f2a-9288-4ef5-aa57-7280523cfd4c",
   "metadata": {},
   "source": [
    "Using anchor points to estimate performance in the test set and reporting the average prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79301f-1140-474e-bd41-832f9c6d0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: harness_truthfulqa_mc_0, avg. error: 0.019\n",
      "scenario: gsm8k, avg. error: 0.025\n",
      "scenario: winogrande, avg. error: 0.026\n",
      "scenario: arc, avg. error: 0.018\n",
      "scenario: hellaswag, avg. error: 0.012\n",
      "scenario: mmlu, avg. error: 0.022\n"
     ]
    }
   ],
   "source": [
    "# Y：（模型数，题目数），每个值为0/1\n",
    "for scenario in scenarios.keys():\n",
    "    # Y_test 形状 （100，题目数量），0/1值\n",
    "    # [:,scenarios_position[scenario]] 第一次筛选，100个模型，和所有对应scenario的题目，使变量变为(100,14042)\n",
    "    # [:,anchor_points[scenario]] 第二次筛选，100个模型，和所有对应scenario的题目中的锚点题目，使变量变为（100，100）\n",
    "    Y_anchor = Y_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    # `Y_hat`: 通过模型在100个代表性题目上的表现，乘以这些题目的重要性权重，最终估算出每个模型的总成绩。\n",
    "    Y_hat = (Y_anchor*anchor_weights[scenario]).sum(axis=1)\n",
    "    Y_true = (balance_weights*Y_test)[:,scenarios_position[scenario]].mean(axis=1)\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(Y_hat-Y_true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e3a2b",
   "metadata": {},
   "source": [
    "## 生成中文 MMLU 的两个相似子集（各 300 题）\n",
    "\n",
    "依据 `mmlu_CN_Prompt.mdc` 的要求：\n",
    "- 只使用英文侧的题目表现信号（此处采用英文侧模型在题目上的平均正确率作为难度 proxy），不使用中文答案信息。\n",
    "- 排除 subjects：`high_school_us_history`, `security_studies`, `high_school_government_and_politics`, `jurisprudence`, `business_ethics`, `us_foreign_policy`, `global_facts`。\n",
    "- 分层标准：按 `Subject` 与难度分位桶（5 桶）联合分层；在每个分层中按整体占比进行抽样，生成两个尽量分布一致且不重叠的 300 题子集。\n",
    "- 输出：\n",
    "  - `../tutorials/mmlu_ZH-CN_subset_1.csv`\n",
    "  - `../tutorials/mmlu_ZH-CN_subset_2.csv`\n",
    "  - 摘要：`../tutorials/mmlu_ZH-CN_subset_summary.json`\n",
    "\n",
    "使用方法：依次从头运行本 Notebook 全部单元，最后执行下方代码单元以生成结果文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec9fdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成:\n",
      "- tutorials/mmlu_ZH-CN_subset_1.csv\n",
      "- tutorials/mmlu_ZH-CN_subset_2.csv\n",
      "- tutorials/mmlu_ZH-CN_subset_summary.json\n",
      "子集大小:  300 300\n",
      "重叠题目数:  0\n",
      "对齐度(L1): {'subject_props_L1': 0.3135, 'difficulty_bucket_props_L1': 0.1933}\n"
     ]
    }
   ],
   "source": [
    "# 基于英文侧题目表现信号，生成两个中文子集各300题，并输出摘要\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# 1) 计算英文侧 MMLU 每题平均正确率（难度 proxy；越低越难）\n",
    "Y_mmlu = Y[:, scenarios_position['mmlu']]\n",
    "item_acc = Y_mmlu.mean(axis=0)\n",
    "\n",
    "# 2) 读取中文题库，按 ID 对齐英文指标\n",
    "cn_path = 'mmlu_ZH-CN.csv'\n",
    "df_cn = pd.read_csv(cn_path)\n",
    "assert df_cn.shape[0] == item_acc.shape[0], '中文题库行数与英文 MMLU 题数不一致'\n",
    "df_cn['acc'] = item_acc[df_cn['ID'].values]\n",
    "\n",
    "# 3) 排除指定 subjects\n",
    "excluded_subjects = set([\n",
    "    'high_school_us_history',\n",
    "    'security_studies',\n",
    "    'high_school_government_and_politics',\n",
    "    'jurisprudence',\n",
    "    'business_ethics',\n",
    "    'us_foreign_policy',\n",
    "    'global_facts',\n",
    "])\n",
    "mask_keep = ~df_cn['Subject'].isin(excluded_subjects)\n",
    "df_pool = df_cn[mask_keep].copy().reset_index(drop=True)\n",
    "\n",
    "# 4) 构造难度分位桶（5 桶）。高 acc 更容易；仅用于分布对齐\n",
    "num_buckets = 5\n",
    "# 若acc取值重复较多，qcut可能会掉桶，允许自动降重\n",
    "df_pool['acc_bucket'] = pd.qcut(df_pool['acc'], q=num_buckets, labels=False, duplicates='drop')\n",
    "\n",
    "# 若桶数不足，降级到按 rank 切分\n",
    "if df_pool['acc_bucket'].isna().any():\n",
    "    # 使用秩近似分桶\n",
    "    ranks = df_pool['acc'].rank(method='average') / (len(df_pool))\n",
    "    df_pool['acc_bucket'] = np.minimum((ranks * num_buckets).astype(int), num_buckets - 1)\n",
    "\n",
    "# 5) 按 Subject x acc_bucket 联合分层抽样，生成两个不重叠子集\n",
    "TOTAL = 300\n",
    "\n",
    "def sample_proportional_stratified(df_source, total, seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    group_cols = ['Subject', 'acc_bucket']\n",
    "    counts = df_source.groupby(group_cols).size()\n",
    "    props = counts / counts.sum()\n",
    "    raw_targets = props * total\n",
    "    base = np.floor(raw_targets).astype(int)\n",
    "    remainder = (raw_targets - base).sort_values(ascending=False)\n",
    "    need = total - base.sum()\n",
    "    # 分配剩余名额\n",
    "    add = pd.Series(0, index=base.index)\n",
    "    if need > 0:\n",
    "        add.loc[remainder.index[:need]] = 1\n",
    "    targets = base + add\n",
    "    # 按每个分层抽样\n",
    "    parts = []\n",
    "    for grp, n in targets.items():\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        sub = df_source[(df_source['Subject'] == grp[0]) & (df_source['acc_bucket'] == grp[1])]\n",
    "        k = min(n, len(sub))\n",
    "        if k > 0:\n",
    "            parts.append(sub.sample(n=k, random_state=rng.randint(0, 1_000_000)))\n",
    "    out = pd.concat(parts, axis=0) if len(parts) else df_source.iloc[0:0]\n",
    "    # 若不足 total，随机从剩余中补齐\n",
    "    short = total - len(out)\n",
    "    if short > 0:\n",
    "        remain = df_source.drop(out.index)\n",
    "        if len(remain) >= short:\n",
    "            out = pd.concat([out, remain.sample(n=short, random_state=rng.randint(0, 1_000_000))], axis=0)\n",
    "        else:\n",
    "            out = pd.concat([out, remain], axis=0)\n",
    "    return out.iloc[:total].sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "subset1 = sample_proportional_stratified(df_pool, TOTAL, random_state)\n",
    "remain_pool = df_pool[~df_pool['ID'].isin(subset1['ID'])].copy()\n",
    "subset2 = sample_proportional_stratified(remain_pool, TOTAL, random_state + 1)\n",
    "\n",
    "# 6) 计算摘要统计\n",
    "\n",
    "def dist_counts_props(series):\n",
    "    counts = series.value_counts().sort_index()\n",
    "    props = (counts / counts.sum()).round(4)\n",
    "    return counts.to_dict(), props.to_dict()\n",
    "\n",
    "sub1_subject_counts, sub1_subject_props = dist_counts_props(subset1['Subject'])\n",
    "sub2_subject_counts, sub2_subject_props = dist_counts_props(subset2['Subject'])\n",
    "sub1_bucket_counts, sub1_bucket_props = dist_counts_props(subset1['acc_bucket'])\n",
    "sub2_bucket_counts, sub2_bucket_props = dist_counts_props(subset2['acc_bucket'])\n",
    "\n",
    "overlap_ids = sorted(list(set(subset1['ID'].tolist()) & set(subset2['ID'].tolist())))\n",
    "\n",
    "# 简单的分布对齐度量（L1差）：\n",
    "def l1_diff(props1, props2):\n",
    "    keys = set(props1.keys()) | set(props2.keys())\n",
    "    return float(sum(abs(props1.get(k, 0.0) - props2.get(k, 0.0)) for k in keys))\n",
    "\n",
    "alignment = {\n",
    "    'subject_props_L1': round(l1_diff(sub1_subject_props, sub2_subject_props), 4),\n",
    "    'difficulty_bucket_props_L1': round(l1_diff(sub1_bucket_props, sub2_bucket_props), 4),\n",
    "}\n",
    "\n",
    "summary = {\n",
    "    'subset_1': {\n",
    "        'size': int(len(subset1)),\n",
    "        'subject_counts': sub1_subject_counts,\n",
    "        'subject_props': sub1_subject_props,\n",
    "        'difficulty_bucket_props': sub1_bucket_props,\n",
    "    },\n",
    "    'subset_2': {\n",
    "        'size': int(len(subset2)),\n",
    "        'subject_counts': sub2_subject_counts,\n",
    "        'subject_props': sub2_subject_props,\n",
    "        'difficulty_bucket_props': sub2_bucket_props,\n",
    "    },\n",
    "    'overlap': {\n",
    "        'count': int(len(overlap_ids)),\n",
    "        'ids': overlap_ids,\n",
    "    },\n",
    "    'alignment': alignment,\n",
    "    'excluded_subjects': sorted(list(excluded_subjects)),\n",
    "}\n",
    "\n",
    "# 7) 输出 CSV 与摘要 JSON\n",
    "out_dir = '../tutorials'\n",
    "subset1[['ID', 'Question', 'A', 'B', 'C', 'D', 'Answer', 'Subject']].to_csv(f'{out_dir}/mmlu_ZH-CN_subset_1.csv', index=False)\n",
    "subset2[['ID', 'Question', 'A', 'B', 'C', 'D', 'Answer', 'Subject']].to_csv(f'{out_dir}/mmlu_ZH-CN_subset_2.csv', index=False)\n",
    "with open(f'{out_dir}/mmlu_ZH-CN_subset_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('生成完成:')\n",
    "print('- tutorials/mmlu_ZH-CN_subset_1.csv')\n",
    "print('- tutorials/mmlu_ZH-CN_subset_2.csv')\n",
    "print('- tutorials/mmlu_ZH-CN_subset_summary.json')\n",
    "print('子集大小: ', len(subset1), len(subset2))\n",
    "print('重叠题目数: ', len(overlap_ids))\n",
    "print('对齐度(L1):', alignment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TinyBenchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
